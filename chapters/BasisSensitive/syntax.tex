\section{The calculus}
\subsection{Syntax}

This section presents the calculus upon which our realizability model will be designed. It is a lambda-calculus extended with linear combinations of lambda-terms, which form a vector space.

The calculus is divided into four distinct syntactic categories: \textit{pure values, pure terms, value distributions and term distributions}. Values are composed by variables, a decorated lambda abstraction and two boolean values representing perpendicular vectors: $\ket 0$ and $\ket 1$. A pair of values is also a value itself. Terms include values, applications, pair constructors and destructors and pattern-matching testing for orthogonal vectors represented by the $\mathsf{case}$ operator. Both terms and value distributions are built by a $\C$-linear combination of either terms or values respectively. In Table \ref{tab:PairsNotation} we also include notation for ease of writing of linear distributions of pairs.


\begin{table*}[t]
  \small
  \[\begin{array}{l@{\quad}rll@{}}
    v&::=& x \mid \Lam{x}B{\vec{t}} \mid (v, v) \mid\ket{0} \mid \ket{1} \\[6pt]
    t&::=& w \mid  t\,t \mid \LetP{x}{B}{y}{B}{\vec{t}}{\vec{t}} 
    \mid\\
    &&\gencase{\vec{t}}{\vec{v}}{\vec{v}}{\vec{t}}{\vec{t}}\\[6pt]
    \vec{v}&::=& v \mid \vec{v}+\vec{v} \mid 
    \alpha\cdot\vec{v}\qquad\hfill(\alpha\in\C)\\[6pt]
    \vec{t}&::=&
    t \mid \vec{t}+\vec{t} \mid 
    \alpha\cdot\vec{t}\qquad\hfill(\alpha\in\C)
   \end{array}
  \]
   
  Where $B$ is an $n$-th dimensional orthonormal basis as defined in Def. \ref{def:NthDimensionalBasis}.

  \caption{Syntax of the calculus}
  \label{tab:Syntax}
\end{table*}

 \begin{table*}[tb]
  \[
    \begin{array}{c}
      \Pair{\alpha\cdot v+\vec{v}_1}{\vec{v}_2}~ := ~\alpha\Pair{v}{\vec{v}_2} + \Pair{\vec{v}_1}{\vec{v}_2}\\
      \Pair{\vec{v}_1}{\alpha\cdot v+\vec{v}_2}~ := ~\alpha\Pair{\vec{v}_1}{v} + \Pair{\vec{v}_1}{\vec{v}_2}
    \end{array}
  \]
  \caption{Notation for writing pair distributions}
  \label{tab:PairsNotation}
 \end{table*}

\begin{remark}
  We do not include a single specific term representing the null vector $\vec{0}$ since we do not make use of it. Instead, any distribution $0\cdot\vec{t}$ will act as one. 
\end{remark}

In order to handle the different bases in each abstraction, we need to define a congruence relationship between values. When we define the reduction system, this congruence will allow us to take an argument an interpret it in the corresponding basis of the function. Here, the vector space of value distributions starts to take shape.

\begin{table*}[tb]
  \small
  \vspace*{0.2cm}
  \[
    \begin{array}{l}
      \vec{v_1} + 0\cdot \vec{v_2}~\equiv~\vec{v_1}\\  
      \qquad\text{ Where $\vec{v_i}$ is neither an abstraction, nor a variable}
    \end{array}   
  \]
  
  \[\renewcommand*{\arraystretch}{1.2}
    \begin{array}{c c}
      1\cdot\vec{t}~\equiv~\vec{t}&
      \alpha\cdot(\beta\cdot\vec{t})~\equiv~\alpha\beta\cdot\vec{t}\\
      \vec{t}_1+\vec{t}_2 ~\equiv~\vec{t}_2+\vec{t}_1 &
      (\vec{t}_1+\vec{t}_2)+\vec{t}_3~\equiv~\vec{t}_1+(\vec{t}_2+\vec{t}_3)\\
      \multicolumn{2}{c}{(\alpha+\beta)\cdot\vec{t}~\equiv~\alpha\cdot\vec{t}+\beta\cdot\vec{t}}\\
      \multicolumn{2}{c}{\alpha\cdot(\vec{t}_1+\vec{t}_2)~\equiv~\alpha\cdot\vec{t}_1+\alpha\cdot\vec{t}_2}\\
      \vec t (\alpha \vec s)~\equiv~ \alpha (\vec{t}\vec{s})&
      (\alpha \vec t) \vec s ~\equiv~ \alpha (\vec{t}\vec{s})\\
      (\vec t + \vec s) \vec{r}~\equiv~\vec{t}\vec{r} + \vec{s}\vec{r} &
      \vec t(\vec{s}+\vec{r})~\equiv~\vec{t}\vec{s} + \vec{t}\vec{r}\\
      \multicolumn{2}{l}{\LetP{x_1}{A_1}{x_2}{B_2}{(\alpha\vec{t})}{\vec{s}}\equiv}\\
      \multicolumn{2}{r}{\alpha(\LetP{x_1}{A_1}{x_2}{B_2}{\vec{t}}{\vec{s}})}\\
      \multicolumn{2}{l}{\LetP{x_1}{A_1}{x_2}{B_2}{\vec{t}+\vec{s}}{\vec{r}}\equiv}\\
      \multicolumn{2}{r}{(\LetP{x_1}{A_1}{x_2}{B_2}{\vec{t}}{\vec{r}})}\\
      \multicolumn{2}{r}{+~(\LetP{x_1}{A_1}{x_2}{B_2}{\vec{s}}{\vec{r}})}\\
      \multicolumn{2}{l}{\gencase{\alpha \vec{t}}{\vec{v_1}}{\vec{v_n}}{\vec{s_1}}{\vec{s_n}}\equiv}\\
      \multicolumn{2}{r}{\alpha(\gencase{\vec{t}}{\vec{v_1}}{\vec{v_n}}{\vec{s_1}}{\vec{s_n}})}\\
      \multicolumn{2}{l}{\gencase{(\vec{t}+\vec{s})}{\vec{v}}{\vec{w}}{\vec{r_1}}{\vec{r_2}}\equiv}\\
      \multicolumn{2}{r}{\gencase{\vec{t}}{\vec{v}}{\vec{w}}{\vec{r_1}}{\vec{r_2}}}\\
      \multicolumn{2}{r}{+~\gencase{\vec{s}}{\vec{v}}{\vec{w}}{\vec{r_1}}{\vec{r_2}}}
    \end{array}
  \]
  \caption{Term congruence}
  \label{tab:Congruence}
\end{table*}


We expand on the rationale for the first rule, $\vec{v_1} + 0\cdot \vec{v_2}~\equiv~\vec{v_1}$. The main idea of the calculus is to decompose the vectors corresponding to the arguments onto the bases attached to the abstractions. Taking an example from linear algebra, if we were to rewrite the vector $(1,0)$ as a linear combination of $\braces{\frac{(1,1)}{\sqrt{2}}, \frac{(1,-1)}{\sqrt{2}}}$ we would get:

\begin{align*}
  (1,0) &= (1,0) + 0 \cdot (0,1) \\
  &=\frac{1}{2} ((1,0) + (1,0) + (0,1) - (0,1))\\
  &=\frac{1}{\sqrt{2}}\cdot\left(\frac{(1,1)}{\sqrt{2}} + \frac{(1,-1)}{\sqrt{2}}\right)  
\end{align*}

If we match the vector $(1,0)$ to $\ket{0}$ and $(0,1)$ to $\ket{1}$, we would need a way to introduce the second coordinate into the equation. That is were the first rule comes into play. We restrict ourselves to vectors, since introducing variables or abstractions could break safety properties of the system.
%ACÁ PUEDO PONER UN EJEMPLO. ID APLICADO A ID Y EN CADA UNO DE LOS TÉRMINOS AGREGO UN DUPLICADOR. EVENTUALMENTE REDUCE A ID+0.OMEGA QUE NO TIENE FORMA NORMAL. 

We consider the congruence $\equiv$ as shallow. This means it does not act under lambda abstractions nor case alternatives. For example, if $\vec{t}\neq\vec{s}$: $(\Lam{x}{B}{\vec{t}}) \not\equiv (\Lam{x}{B}{\vec{s}})$ (Even in the case that $\vec{t}\equiv\vec{s}$).

% ACÁ EXPLICAR QUE LA RAZÓN QUE SEA SHALLOW ES PARA TENER CANONICITY. ES DECIR QUE LA DESCRIPCIÓN DE UN CIRCUITO QUE GENERA UN QUBIT QUEDA FIJA. 

% En la parte de conclusiones/trabajo futuro, me gustaría meter otra noción de producto interno que juegue bien con las funciones. Con esta definición una función es ortogonal a su eta expansión. O (\x. t_1 + t_2) _|_ (\x. t_2 + t_1)

The core mechanism of the calculus lies in decorating variable bindings with sets of value distributions. Keeping with linear algebra terminology, we will refer to this sets as \textit{(orthonormal) bases}, for reasons which will shortly become clear. These bases will inform the reduction system on how to operate its arguments. 

In order to properly characterize the sets that decorate the lambda abstractions, we first have to define which are the values that they must contain.
% Definición de qubit
\begin{definition}
  A $1$-dimensional qubit is a value distribution of the form: $\alpha \ket{0} + \beta \ket{1}$ where $|\alpha|^2 + |\beta|^2 = 1$. An $n$-th dimensional qubit is a value distribution of the form $\alpha\Pair{\ket{0}}{\vec w_1} + \beta\Pair{\ket{1}}{\vec{w_2}} $ where $\vec w_1$ and $\vec{w_2}$ are $(n-1)$ dimensional qubits and the same previous conditions apply to $\alpha$ and $\beta$.
\end{definition}

% Definición de producto interno, ortogonalidad e independencia lineal. 
From this point forward we shall write $\vec{\Val}$ to the space of all closed value distributions which we will call \emph{vectors}. This space is equipped with an inner product $\scal{\vec v}{\vec w}$ and a pseudo-$\ell_2-norm$ $\|\vec v\|$ defined as:

\begin{align*}
  \textstyle\scal{\vec{v}}{\vec{w}}&:=\textstyle\sum_{i=1}^n\sum_{j=1}^m\overline{\alpha_i}\,\beta_j\,\delta_{v_i,w_j}\\
  \textstyle\|\vec{v}\,\|&:=\textstyle\sqrt{\scal{\vec{v}}{\vec{v}\,}}=\textstyle\sqrt{\sum_{i=1}^n|\alpha_i|^2}    
\end{align*}

Where $\vec{v}=\sum_{i=1}^n\alpha_i\cdot v_i$ and $\vec{w}=\sum_{j=1}^m\beta_j\cdot w_j$, and where $\delta_{v_i,w_j}$ is the Kronecker delta such that it is $1$ if $v_i=w_j$ and $0$ otherwise.

With the notion of an internal product, we can finalize the details on the calculus syntax. As one might expect, we will say two values are orthogonal when their internal product equals to zero. With the previous definition we can describe the sets decorating the abstractions.

\begin{definition}\label{def:NthDimensionalBasis}
We will say a set of value distributions $B$ is an $n$-th dimensional orthonormal basis when it satisfies the following conditions:
\begin{enumerate}
  \item Each member of $B$ is a qubit of dimension $n$.
  \item Each member has norm equal to $1$.
  \item Each member of $B$ is pairwise orthogonal to every other member. 
\end{enumerate}
\end{definition}

Unlike the usual definition of orthonormal basis, we also need to ensure that the members are qubits. In other words, they are neither variables nor abstractions. Morally, these sets will keep track of the basis the term is working on. A qubit which is a member of this set will be treated on a call-by-value strategy and its data can be treated clasically. Any other qubit will first be interpreted as a $\C$-linear combination of elements of the basis and then the function will apply linearly to each component. If the argument cannot be written in the decorating basis, the evaluation gets stuck. 

\subsection{Substitutions}

The beta reduction will depend on the basis chosen for the abstraction, so we have to define a new subsitution which will take this mechanism into account. This operation will substitute the variables for vectors in the chosen basis. The accompanying coefficents correspond to the value distribution which is the object of the substitution.
  
With this substitution we also define a special kind of basis wich we call $\AbsBasis$ which will act as the canonical basis for lambda abstractions. In this way, we restrict distributions of functions to a single possible basis.

\begin{definition}
  For a term distribution $\vec{t}$, value distribution $\vec{v}$, variable $x$ and orthogonal basis $A$, we define the substitution $\vec{t}\ansubst{\vec{v}/x}{A}$ as:
  
  \[
  \vec{t}\ansubst{\vec{v}/x}{A} = 
    \begin{cases}
      \sum_{i\in I} \alpha_i \vec t\ [\vec b_i / x] 
      & A=\{\vec{b_i}\}_{i\in I}\wedge\vec{v}\equiv\sum\limits_{i\in I} \alpha_i \vec{b_i} \\
      \sum_{i\in I}\alpha_i\vec{t}\ [v_i/x] & A = \AbsBasis\wedge\vec{v}=\sum\limits_{i\in I}\alpha_i v_i\\
      \text{Undefined} & \text{Otherwise}
    \end{cases}
  \]

  We extend the substitution for more than one pair of variables. This definition is extended to pair of values in the following way. Let $\vec v = \sum_{i\in I} \alpha_i \Pair{\vec{v_i}}{\vec{w_i}}$:
  \[
    \vec t\ansubst{\vec{v}/x\otimes y}{A \otimes B} = \sum_{i\in I} \alpha_i \vec t\ansubst{\vec{v_i}/x}{A}\ansubst{\vec{w_i}/y}{B}
  \]
\end{definition}

With this new substitution defined, we set out to prove some lemmas which will be useful later for proving the validity of some typing judgements. First of all, we want to show that the basis dependent substitution commutes with the linear combination of terms.

\begin{lemma}\label{lem:distributiveSubstitution}
  $(\sum_i \alpha_i\vec t_i)\ansubst{\vec v/x}{A} \equiv \sum_i \alpha_i\vec t_i \ansubst{\vec v/x}{A}$ 
\end{lemma}

\begin{proof}
  Let $\vec{v}\equiv\sum\limits_{j=0}^n \beta_j \vec{v_j}$
  \begin{align*}
    (\sum_i \alpha_i\vec t)\ansubst{\vec v/x}{A} &= \sum_{j=1}^m \beta_j(\sum_{i=1}^{n} \alpha_i t_i)[\vec v_j/x]\\
    &\equiv \sum_{i=1}^{n} \alpha_i (\sum_{j=1}^m \beta_j t_i [\vec v_j/x])\\
    &= \sum_{i=1}^{n} \alpha_i \vec{t_i}\ansubst{\vec v/x}{A}
  \end{align*}
\end{proof}

The next thing we need to show is that the substitution behaves well with respect to the term congruence previously defined. In essence, the following results states that for each member of the same equivalence class defined by $\equiv$, the result of substitution for those vectors is always sintactically the same.

\begin{lemma}\label{lem:EquivSubstitutions}
  If $\vec{v}\equiv\vec{w}$, then $\vec{t}\ansubst{\vec{v}/x}{A}=\vec{t}\ansubst{\vec{w}/x}{A}$.
\end{lemma}

\begin{proof}
  Since $\vec{v}\equiv\vec{w}$, by corollary \ref{cor:EquivalentDecomposition}, we have that both $\vec{v}$ and $\vec{w}$ can be written as:
  \[
  \vec{v} \equiv \vec{w} \equiv \sum_{i=1}^{n} \alpha_i \vec{a_i}\qquad\text{Where }\vec{a_i}\in A
  \]
  Then:
  \[
  \vec{t}\ansubst{\vec{v}/x}{A} = \sum_{i=1}^{n} \alpha_i \vec{t}[\vec{a_i}/x] = \vec{t}\ansubst{\vec{w}/x }{A}
  \]
\end{proof}

\begin{remark}
  The result from lemma \ref{lem:EquivSubstitutions}, does not translate across bases, so $\vec{t}\ansubst{\vec{v}/x}{A} \not\equiv \vec{t}\ansubst{\vec{v}/x}{B}$. From here onwards we will define $\ket{+}:=\frac{\ket{0}+\ket{1}}{\sqrt{2}}$ and $\ket{-}:=\frac{\ket{0}-\ket{1}}{\sqrt{2}}$. As well we will define, $\B = \{\ket{0}, \ket{1}\}$ and $\XB = \{\ket{+}, \ket{-}\}$. With this we have:
  
  \begin{align*}
    (\Lam{x}{C}{y})\ansubst{\ket{+}/y}{\XB} &= (\Lam{x}{C}{\ket{+}}) 
    \not\equiv \\ 
    &\frac{1}{\sqrt{2}} ((\Lam{x}{C}{\ket{0}}) + (\Lam{x}{C}{\ket{1}}))
    = (\Lam{x}{C}{y})\ansubst{\ket{+}/y}{\B}
  \end{align*}
  
  This boils down to the fact that the $\equiv$-relationship does not commute neither with the lambda abstraction nor the case construct. This is due to te fact that, despite being computationally equivalent, the terms $\Lam{x}{X}{\sum_{i=1}^{n}\alpha_i \vec{t_i}}$ and $\sum_{i=1}^{n}\alpha_i \Lam{x}{X}{\vec{t_i}}$ are not congruent (Similarly for the case construct)
\end{remark}

{\color{red} PONER ACÁ LAS DEFINICIONES DEL ÁLGEBRA DE SUSTITUCIONES}

\iffalse
%Este lema no estoy seguro de donde ponerlo. Es posible que lo corte
A question that might arise from the previous remark could be if taking an $\eta$-expansion on a different basis would yield different a result. In this particular instance, it is fairly simple to see that it is not the case.

\begin{lemma}[$\eta$-expansion]
  For every $\lambda$-abstraction $(\Lam{x}{\basis{X}}{\vec t})$ where $y\not\in\FV{\vec{t}}$, value distribution $\vec{v}\in\sem{\sharp{\basis{X}}}$ and .
  
  \[
    (\Lam{y}{\basis{Y}}{(\Lam{x}{\basis{X}}{\vec t})\ y})\ \vec{v}\evalone (\Lam{x}{\basis{X}}{\vec{t}})\ \vec v
  \]
\end{lemma}

\begin{proof}
  Let $\vec{v}\equiv\sum_{i=1}^{n} \alpha_i \vec{b_i}$ where $\vec{b_i}\in\sem{\basis{Y}}$, we have that:
  \begin{align*}
    (\Lam{y}{\basis{Y}}{(\Lam{x}{\basis{X}}{\vec{t}})}\ y)\ \vec{v}&\evalone(\Lam{x}{\basis{X}}{\vec{t}})\ansubst{\vec{v}/y}{\basis{Y}}\\ 
    &= \sum_{i=1}^{n} \alpha_i (\Lam{x}{\basis{X}}{\vec{t}}) \vec{b_i}\\ 
    &\equiv (\Lam{x}{\basis{X}}{\vec{t}})(\sum_{i=1}^{n}\alpha_i \vec{b_i})\\
    &\equiv (\Lam{x}{\basis{X}}{\vec{t}})\vec{v}
  \end{align*}
\end{proof}
\fi