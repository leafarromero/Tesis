\section{Reduction system}\label{sec:reduction}

The reduction system implements a mechanism where every vector in the space is read in the corresponding basis attached to the abstraction. It does this by allowing an evaluation step only when the argument can be decomposed onto that basis. The system works modulo the congruence defined in Table \ref{tab:Congruence}. We describe it in detail in Table \ref{tab:Reduction}.

\begin{table*}[tb]
  \small
  \begin{align*}
    \sum_{i=1}^{n}\alpha_i(\Lam{x}{A}{\vec{t_i}})\ \vec{v} &\evalone \sum_{i=1}^{n} \alpha_i \vec{t_i}\ansubst{\vec v/x}{A}\\[5pt]
    \text{If }&\vec{t_i}\ansubst{\vec v/x}{A}\text{ is defined}\\
    \LetP{x}{B}{y}{B'}{\vec v}{\vec{t}} & \evalone\vec{t}\ansubst{\vec{v}/x\otimes y}{B\otimes B'}\\[5pt]
    \gencase{\vec{v}}{\vec{v_1}}{\vec{v_n}}{\vec{t_1}}{\vec{t_2}}
    &\evalone\sum_{i=1}^{n} \alpha_i \vec{t_i}\\
    \text{If }&\vec{v}\equiv\sum_{i=1}^{n}\alpha_i \vec{v_i}
  \end{align*}
  
  \[
  \begin{array}{c c c}
    \infer{s\,t\lra s\,\vec r}{t\lra \vec r}
      &
      \infer{t\,v\lra r\,v}{t\lra r}
      &
      \infer{\alpha\cdot t+\vec s\lra\alpha\cdot\vec r+\vec s}{t\lra\vec r}\\[5pt]
      \multicolumn{3}{c}{
      \infer{\LetP{x}{A}{y}{B}{t}{\vec{s}}\lra \LetP{x}{A}{y}{B}{\vec r}{\vec{s}}}{t\lra \vec r}}
      \\[5pt]
  \end{array}
  \]

  \[
  \begin{array}{c}
      \infer
      {\begin{array}{c c}
        \gencase{\vec t}{\vec v}{\vec w}{\vec s_1}{\vec s_2}\lra&\\
        \multicolumn{2}{r}{\gencase{\vec r}{\vec v}{\vec w}{\vec s_1}{\vec s_2}}
      \end{array}
      }
      {t\lra \vec r}
  \end{array}
  \]
  \caption{Reduction system}
  \label{tab:Reduction}
\end{table*}

The three main rules are the $\beta$-reduction, $\mathsf{let}$-destructor and $\mathsf{case}$ pattern matching. The $\lambda$ abstraction and $\mathsf{let}$ construct both attach an orthonormal basis to the variables they are binding. These bases keep track of which vectors it considers as classical data. Any $\C$-combination of them will be treated as quantum data, meaning, linearly. 

The only exception is in the case of higher order reductions. Since we do not have defined orthogonal bases for programs, we introduce a special basis $\AbsBasis$ which acts as the traditional computational basis. We can think of it as being composed of every single pure value. For example:
%PENSAR Y REEMPLAZAR CON ALGÚN EJEMPLO MÁS CONCRETO E INTERESANTE.
\begin{align*}
  \sum_{i=1}^{n}\alpha_i(\Lam{x}{\AbsBasis}{\vec{t_i}}) \sum_{j=1}^{m}\beta_j&(\Lam{y}{\basis{X}}{\vec{s_j}}) \evalone\\
  &\sum_{i=1}^n\sum_{j=1}^{m}\alpha_i\beta_j \vec{t_i}[(\Lam{y}{\basis{X}}{\vec{s_j}})/x]
\end{align*}

The $\mathsf{case}$ pattern matching controls the flow of programs. It generalizes the $\mathsf{if-then-else}$ branching. However, we do not consider fixed true or false values. Each operator will keep track of a set of orthogonal values. Then it will test the argument for equality against each vector and choose the matching branch. If the argument is a linear combination of several vectors, the result will be the corresponding linear combination of branches. For example:

\[
  \case{\ket{-}}{\ket{0}}{\ket{1}}{\vec{t_1}}{\vec{t_2}} \evalone
  \frac{1}{\sqrt{2}}\cdot\vec{t_1} - \frac{1}{\sqrt{2}}\cdot\vec{t_2}
\]

The advantage of this general approach over a binary conditional is the possibility to match against several vectors simultaneously. For boolean tuples, it makes no difference since we can treat each component independently. However, there are orthogonal bases which cannot be written as the product of two smaller bases themselves. In this case, the general $\mathsf{case}$ allows us match against these vectors. For example:

\begin{align*}
  \mathsf{case}\ \vec{v}\ \mathsf{of}\ \{ 
  &\frac{\ket{00} + \ket{11}}{2}\mapsto \vec{t_1} \mid\\
  &\frac{\ket{00} - \ket{11}}{2}\mapsto \vec{t_2} \mid\\
  &\frac{\ket{01} + \ket{10}}{2}\mapsto \vec{t_3} \mid\\
  &\frac{\ket{01} - \ket{10}}{2}\mapsto \vec{t_4} \}
\end{align*}

This particular set of four vectors is called the \textit{Bell basis}. It is useful in the field of quantum communication. In a later section, we will explore the quantum teleportation algorithm which heavily relies on these states. 

Defining the system in this way determines a strategy in the \textit{call-by-value} family, which we dub \textit{call-by-arbitrary-basis}. Note that evaluation is weak, meaning that no reduction occurs under lambda, pairs, let or conditional constructors.

The congruence relation on terms gives rise to different redexes. However, we can show that the relation $\equiv$ commutes with the reflexive-transitive closure of the reduction $\evalone$ (We shall note $\eval$ as this reflexive-transitive closure). In other words, equivalence is preserved by the reduction $\eval$.

\begin{theorem}[Congruence modulo-$\equiv$]
  Let $\vec{t}$ and $\vec{s}$ be closed term distributions such that $\vec{t}\equiv\vec{s}$. If $\vec{t}\evalone\vec{t'}$, and $\vec{s}\evalone\vec{s'}$. Then there exists term distributions $\vec{r_1},\vec{r_2}$ such that $\vec{t'}\eval\vec{r_1}$, $\vec{s'}\eval\vec{r_2}$ and $\vec{r_1}\equiv\vec{r_2}$. Diagrammatically:

\[
  \begin{tikzcd}
   & \vec{t} \arrow[ld]&[-2.5em] \equiv &[-2.5em] \vec{s}\arrow[rd] &\\
   \vec{t'}\arrow[dr, twoheadrightarrow] & & & & \vec{s'}\arrow[ld, twoheadrightarrow] \\
   & \vec{r_1} & \equiv & \vec{r_2} &
  \end{tikzcd}
\]

\end{theorem}
\begin{proof}
  Induction over the relation $\equiv$. 
  \begin{description}
    \item[$\vec{t} (\alpha\vec{s})\equiv\alpha(\vec{t}\vec{s})$:] If either $\vec{t}$ or $\vec{s}$ reduce, then merely reduce on both sides of the congruence. If $\vec{t} = \sum_{i=1}^{n}\beta_i (\Lam{x}{A}{\vec t_i})$ and $\vec{s}=\vec v= \sum_{j=1}^{m} \delta_j \vec{a_j}$ for $\vec{a_j}\in A$, then:
    \begin{align*}
      (\sum_{i=1}^{n}\beta_i (\Lam{x}{A}{\vec{t_i}})) (\alpha\vec{v})&\evalone\sum_{i=1}^{n}\beta_i \vec{t_i}\ansubst{\alpha\vec{v}/x}{A}\\
      &=\sum_{i=1}^{n}\beta_i \sum_{j=1}^{m}\alpha\delta_j \vec{t_i}[\vec{a_j}/x]
    \end{align*}
    On the other side:
    \begin{align*}  
    \alpha((\sum_{i=1}^{n}\beta_i (\Lam{x}{A}{\vec{t_i}}))\vec v)&\evalone
    \alpha(\sum_{i=1}^{n}\beta_i \vec{t_i}\ansubst{\vec{v}/x}{A})\\
    &=\alpha(\sum_{i=1}^{n}\beta_i\sum_{j=1}^{m}\delta_j\vec{t_i}[\vec{a_j}/x])
    \end{align*}

    And we have that: 
    \[\sum_{i=1}^{n}\beta_i \sum_{j=1}^{m}\alpha\delta_j \vec{t_i}[\vec{a_j}/x] \equiv \alpha(\sum_{i=1}^{n}\beta_i\sum_{j=1}^{m}\delta_j\vec{t_i}[\vec{a_i}/x])\]

    \item[$(\alpha\vec{t})\vec{s}\equiv\alpha(\vec{t}\vec{s})$:] If either $\vec{t}$ or $\vec{s}$ reduce, then merely reduce them on both sides of the congruence. If $\vec{t} = \sum_{i=1}^{n}\beta_i (\Lam{x}{A}{\vec t_i})$ and $\vec{s}=\vec v$ with $\vec{t_i}\ansubst{\vec{v}/x}{A}$ defined, then:
    \[
    (\alpha\sum_{i=1}^{n}\beta_i (\Lam{x}{A}{\vec{t_i}}))\vec v \evalone
    \alpha\sum_{i=1}^{n}\beta_i \vec{t_i}\ansubst{\vec v/x}{A}
    \]
    
    On the other side:
    \[  
      \alpha(\sum_{i=1}^{n}\beta_i (\Lam{x}{A}{\vec{t_i}})\vec v) \evalone
      \alpha\sum_{i=1}^{n}\beta_i \vec{t_i}\ansubst{\vec v/x}{A}
    \]

    \item[$(\vec{t}+\vec{s})\vec{r}\equiv \vec{t}\vec{s} + \vec{t}\vec{r}$:] There are two cases to consider. If there is a reduction on $\vec t,\vec s$ or $\vec r$, then we have to merely apply the same reduction on both sides of the congruence. If $\vec t=\sum_{i=1}^{n}\alpha_i (\Lam{x}{A}{\vec t_i})$, $\vec s=\sum_{j=1}^{m}\beta_j $ and $\vec r= \vec v$ then:
    
    \begin{align*}
    (\sum_{i=1}^{n}\alpha_i (\Lam{x}{A}{\vec t})+\sum_{j=1}^{m}\beta_j(\Lam{x}{A}{\vec{s_j}}))\vec{v} &\evalone\\
    \sum_{i=1}^{n}\alpha_i\vec{t_i}\ansubst{\vec{v}/x}{A} &+ \sum_{j=1}^{m}\beta_j\vec{s_j}\ansubst{\vec{v}/x}{A}
    \end{align*}

    On the other side:
    \begin{align*}
    (\sum_{i=1}^{n}\alpha_i (\Lam{x}{A}{\vec t})) \vec{v}+(\sum_{j=1}^{m}\beta_j(\Lam{x}{A}{\vec{s_j}}))\vec{v} &\eval\\
    \sum_{i=1}^{n}\alpha_i\vec{t_i}\ansubst{\vec{v}/x}{A} &+ \sum_{j=1}^{m}\beta_j\vec{s_j}\ansubst{\vec{v}/x}{A}
    \end{align*}

    \item[$\vec{t}(\vec{s}+\vec{r})\equiv\vec{t}\vec{s} + \vec{t}\vec{r}$:] There are two cases to consider. If there is a reduction on $\vec{t},\vec{s}$ or $\vec{r}$, then we have to merely apply the same reduction on both sides of the congruence. If $\vec{t}=\sum_{i=1}^{n}\alpha_i(\Lam{x}{A}{\vec{t_i}})$, $\vec{s}=\vec{v}\equiv\sum_{j=1}^{m}\beta_j \vec{a_j}$ and $\vec{r}=\vec{w}\equiv\sum_{j=1}^{m}\delta_j \vec{a_j}$ with $\vec{a_j}\in A$. Then:
    \begin{align*}
    (\sum_{i=1}^{n}\alpha_i(\Lam{x}{A}{\vec{t_i}}))(\vec{v}+ \vec{w}) &\eval \sum_{i=1}^{n}\alpha_i\vec{t_i}\ansubst{\vec{v}+\vec{w}}{A}\\
    &=\sum_{i=1}^{n}\alpha_i \sum_{j=1}^{m} (\beta_j+\delta_j) \vec{t_i}[\vec{a_j}/x]
  \end{align*}
    On the other side:
    \begin{align*}
      (\sum_{i=1}^{n}\alpha_i(\Lam{x}{A}{\vec{t_i}}))\vec{v} + (\sum_{i=1}^{n}\alpha_i(\Lam{x}{A}{\vec{t_i}}))\vec{w} &\eval\\
      \sum_{i=1}^{n}\alpha_i\vec{t_i}\ansubst{\vec{v}/x}{A} + \sum_{i=1}^{n}&\alpha_i\vec{t_i}\ansubst{\vec{w}/x}{A}\\ 
      =\sum_{i=1}^{n}\alpha_i\sum_{j=1}^{m}\beta_j \vec{t_i}[\vec{a_j}/x] + \sum_{i=1}^{n}&\alpha_i\sum_{j=1}^{m}\delta_j\vec{t_i}[\vec{a_j}/x]\\
    \end{align*}
    And we have that:
    \begin{align*}
    \sum_{i=1}^{n}\alpha_i \sum_{j=1}^{m} (\beta_j+\delta_j) \vec{t_i}[\vec{a_j}/x]&\equiv\\
    \sum_{i=1}^{n}\alpha_i\sum_{j=1}^{m}\beta_j \vec{t_i}[\vec{a_j}/x] &+ \sum_{i=1}^{n}\alpha_i\sum_{j=1}^{m}\delta_j\vec{t_i}[\vec{a_j}/x]
    \end{align*}

    \item[$\LetP{x_1}{A}{x_2}{B}{(\alpha \vec{t})}{\vec{s}}\equiv\alpha(\LetP{x_1}{A}{x_2}{B}{\vec{t}}{\vec{s}})$:] \hfill\\
    There are two cases to consider. If there is a reduction on $\vec{t}$, then we have to merely apply the same reduction on both sides of the congruence. If $\vec{t}=\vec{v}=\Pair{\vec{w}}{\vec{u}}$, then:
    \begin{align*}
      \LetP{x_1}{A}{x_2}{B}{(\alpha\vec{v})}{\vec{s}}&\evalone\vec{s}\ansubst{\alpha\vec{v}/ x_1\otimes x_2}{A\otimes B}\\
      &= \alpha \vec{s}\ansubst{\vec{w}/ x_1}{A}\ansubst{\vec{u}/ x_2}{B}
    \end{align*}
    On the other side:
    \begin{align*}
      \alpha(\LetP{x_1}{A}{x_2}{B}{\vec{v}}{\vec{s}})&\evalone
      \alpha (\vec{s}\ansubst{\vec{v}/ x_1\otimes x_2}{A\otimes B})\\
      &=\alpha(\vec{s}\ansubst{\vec{w}/ x_1}{A}\ansubst{\vec{u}/ x_2}{B})
    \end{align*}
    And we have that:
    \[
      \alpha\vec{s}\ansubst{\vec{w}/ x_1}{A}\ansubst{\vec{u}/ x_2}{B}\equiv \alpha(\vec{s}\ansubst{\vec{w}/ x_1}{A}\ansubst{\vec{u}/ x_2}{B})
    \]
    
    \item[\parbox{\linewidth}{\begin{align*}
    &\LetP{x_1}{A}{x_2}{B}{\vec{t}+\vec{s}}{\vec{r}}\equiv\\
    &(\LetP{x_1}{A}{x_2}{B}{\vec{t}}{\vec{r}}) +
    (\LetP{x_1}{A}{x_2}{B}{\vec{s}}{\vec{r}})
    \end{align*}}:]\hfill\\
    
    There are two cases to consider. If there is a reduction on either $\vec{t}$ or $\vec{s}$, then we have to merely apply the same reduction on both sides of the congruence. When $\vec{t}=\vec{u_1}\equiv\sum_{i=1}^{n}\alpha_i\Pair{\vec{v_i}}{\vec{w_i}}$, and $\vec{s}=\vec{u_2}\equiv\sum_{i=1}^{n}\beta_i\Pair{\vec{v_i}}{\vec{w_i}}$ where $\vec{v_i}\in A$ and $\vec{w_i}\in B$. Then:
    \begin{align*}
      \LetP{x_1}{A}{x_2}{B}{\vec{u_1}+\vec{u_2}}{\vec{r}}&\evalone\vec{r}\ansubst{\vec{u_1}+\vec{u_2}/x_1\otimes x_2}{A\otimes B}\\
      &\sum_{i=1}^{n} (\alpha_i + \beta_i) \vec{r}[\vec{v_i}/x_1][\vec{w_i}/x_2]
    \end{align*}
    On the other side:
    \begin{align*}
      (\LetP{&x_1}{A}{x_2}{B}{\vec{u_1}}{\vec{r}})+(\LetP{x_1}{A}{x_2}{B}{\vec{u_2}}{\vec{r}})\\
      &\eval\vec{r}\ansubst{\vec{u_1}/x_1\otimes x_2}{A\otimes B}\ansubst{\vec{u_2}/x_1\otimes x_2}{A\otimes B}\\
      &=\sum_{i=1}^{n}\alpha_i \vec{r}[\vec{v_i}/x_1][\vec{w_i}/x_2] + \sum_{i=1}^{n}\beta_i\vec{r}[\vec{v_i}/x_1][\vec{w_i}/x_2]
    \end{align*}
    And we have that:
    \begin{align*}
      \sum_{i=1}^{n}(\alpha_i + \beta_i) &\vec{r}[\vec{v_i}/x_1][\vec{w_i}/x_2]\equiv\\
      &\sum_{i=1}^{n}\alpha_i \vec{r}[\vec{v_i}/x_1][\vec{w_i}/x_2] + \sum_{i=1}^{n}\beta_i \vec{r}[\vec{v_i}/x_1][\vec{w_i}/x_2]
    \end{align*}

  \item[$\gencase{\alpha \vec{t}}{\vec{v}}{\vec{w}}{\vec{s_1}}{\vec{s_n}}\equiv
  \alpha(\gencase{\vec{t}}{\vec{v}}{\vec{w}}{\vec{s_1}}{\vec{s_n}})$:] 
  There are two cases to consider. If there is a reduction on $\vec{t}$ then we merely apply the same reduction on both sides of the congruence. If $\vec{t}=\vec{u}\equiv\sum_{i=1}^{n}\beta_i \vec{v_i}$, with $\sum_{i=1}^{n}|\beta_i|^2=1$, then:
  \[
  \gencase{\alpha \vec{u}}{\vec{v}}{\vec{w}}{\vec{s_1}}{\vec{s_2}}\evalone
  \sum_{i=1}^{n}\alpha\beta_i \vec{s_i}
  \]
  On the other side:
  \[
  \alpha(\gencase{\vec{u}}{\vec{v}}{\vec{w}}{\vec{s_1}}{\vec{s_2}})\evalone
  \alpha(\sum_{i=1}^{n}\beta_i \vec{s_i})
  \]
  And we have that $\sum_{i=1}^{n}\alpha\beta_i \vec{s_i}\equiv\alpha(\sum_{i=1}^{n}\beta_i \vec{s_i})$.

  \item[\parbox{\linewidth}{$\gencase{(\vec{t}+\vec{s})}{\vec{v}}{\vec{w}}{\vec{r_1}}{\vec{r_n}}\equiv\\ \gencase{\vec{t}}{\vec{v_1}}{\vec{v_n}}{\vec{r_1}}{\vec{r_n}}+\\
  \gencase{\vec{s}}{\vec{v_1}}{\vec{v_n}}{\vec{r_1}}{\vec{r_n}}$:}] \hfill\\
  
  There are two cases to consider. If there is a reduction on either $\vec{t}$ or $\vec{s}$ then we merely apply the same reduction on both sides of the congruence. If $\vec{t}=\vec{u_1}\equiv \sum_{i=1}^{n}\alpha_i \vec{v_1}$, $\vec{s}=\vec{u_2}\equiv\sum_{i=1}^{n}\beta_i \vec{v_i}$, then:
  \begin{align*}
  \gencase{(\vec{u_1}+\vec{u_2})}{\vec{v_1}}{\vec{v_n}}{\vec{r_1}}{&\vec{r_n}}\eval\\
  &\sum_{i=1}^{n} \alpha_i \beta_i \vec{r_i}
  \end{align*}
  On the other side:
  \begin{align*}
    &\gencase{\vec{t}}{\vec{v_1}}{\vec{v_n}}{\vec{r_1}}{\vec{r_n}}+\\
    &\gencase{\vec{s}}{\vec{v_1}}{\vec{v_n}}{\vec{r_1}}{\vec{r_n}}\eval\sum_{i=1}^{n}\alpha_i\vec{r_i} + \sum_{i=1}^{n}\beta_i \vec{r_i}
  \end{align*}
  And we have that $\sum_{i=1}^{n} \alpha_i\beta_i\vec{r_i}\equiv\sum_{i=1}^{n}\alpha_i\vec{r_i} + \sum_{i=1}^{n}\beta_i \vec{r_i}$.
  \end{description}
\end{proof}

\begin{convention}
  With the previous result in mind, we will consider term distributions modulo the $\equiv$ congruence. This will not affect distributions under $\lambda$-abstractions or case conditionals which we only consider up to $\alpha$-conversion.
\end{convention}
